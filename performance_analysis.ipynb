{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b070a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ccc49ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../CODES 20250616T141033_raw_data_20240601_20250616.xlsx\", sheet_name=\"belgian_filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d57b2c",
   "metadata": {},
   "source": [
    "## User Confirmation Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b00d8",
   "metadata": {},
   "source": [
    "### Attempt-level confirmation rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574481e",
   "metadata": {},
   "source": [
    "The percentage of individual attempts that were confirmed by the user. Each row is treated separately in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "65f41ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.16831683168317"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirmed_attempts = (df['IsUserConfirmed'] == True).sum()\n",
    "total = df['IsUserConfirmed'].count()\n",
    "attempt_conf_rate = confirmed_attempts / total * 100\n",
    "attempt_conf_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9692a40",
   "metadata": {},
   "source": [
    "Users who make many repeated attempts can skew the metric.\n",
    "It doesn’t tell you if the same question was eventually confirmed later.\n",
    "It mixes model performance with user persistence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513289e1",
   "metadata": {},
   "source": [
    "### Question-level confirmation rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc13eba",
   "metadata": {},
   "source": [
    "The percentage of unique question instances (per user/session) where the user eventually confirmed the model’s output - even if it took multiple attempts. This is grouped by user and by question creating unique user-question pairs. For example, User 1 - Q1 and User1 Q2 would be treated as separate cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4f1f755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.48780487804879"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_level = (\n",
    "    df.groupby(['SurveySessionID', 'QuestionID'])['IsUserConfirmed']\n",
    "      .max()  # True if any attempt was confirmed\n",
    ")\n",
    "\n",
    "question_conf_rate = (question_level == True).sum() / question_level.count() * 100\n",
    "question_conf_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7683e3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       82\n",
       "unique       2\n",
       "top       True\n",
       "freq        66\n",
       "Name: IsUserConfirmed, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_level.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff05d232",
   "metadata": {},
   "source": [
    "### User-level confirmation rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ff83c",
   "metadata": {},
   "source": [
    "The percentage of users who confirmed at least one model suggestion during their session (single or multiple questions answered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9747f7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.21052631578947"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_level = (\n",
    "    df.groupby('SurveySessionID')['IsUserConfirmed']\n",
    "      .max()  # True if any attempt for any question was confirmed\n",
    ")\n",
    "\n",
    "user_conf_rate = (user_level == True).sum() / user_level.count() * 100\n",
    "user_conf_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f777f",
   "metadata": {},
   "source": [
    "### First-attempt confirmation rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29601d3",
   "metadata": {},
   "source": [
    "The percentage of first attempts per question and user that were confirmed by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6b7057b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.21951219512195"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_attempts = df[df['AttemptID'] == 1]\n",
    "confirmed_first_attempts = first_attempts['IsUserConfirmed'].sum()\n",
    "total = first_attempts['IsUserConfirmed'].count()\n",
    "\n",
    "first_attempt_conf_rate = confirmed_first_attempts / total * 100\n",
    "first_attempt_conf_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a92b1",
   "metadata": {},
   "source": [
    "## AI Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9969b10a",
   "metadata": {},
   "source": [
    "### AI main category classification accuracy (based on the first attempts and after dropping incorrect user responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51565992",
   "metadata": {},
   "source": [
    "First attempts were used to minimise the bias from uneven number of attempts per case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "447df03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.7536231884058"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapsed_data = df[\n",
    "    (df['AttemptID'] == 1) &\n",
    "    (df['MainCategoryCorrect'] != 'UserInputIncorrect')\n",
    "]\n",
    "\n",
    "correct_n = (collapsed_data['MainCategoryCorrect'] == 'Yes').sum()\n",
    "total_n = (collapsed_data['MainCategoryCorrect']).count()\n",
    "\n",
    "main_accuracy = correct_n / total_n * 100\n",
    "main_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "883b665e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainCategoryCorrect\n",
       "Yes                         64\n",
       "Not sure/Old label used      2\n",
       "Not sure/\"Unknown\" label     1\n",
       "Not sure/But seems close     1\n",
       "No/But was correct later     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapsed_data['MainCategoryCorrect'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a2d1d",
   "metadata": {},
   "source": [
    "Not sure/Old label used -> 2 cases had a main label that is no longer in use, so it might to be clear if it was the best option (but still seems reasonable)\n",
    "\n",
    "Not sure/\"Unknown\" label -> user input was: create less trafic, model classification was: Unknown / Other\n",
    "\n",
    "Not sure/But seems close ->   user input was: create sustainable migration channels\n",
    "                              create sustainable and fair migration systems\n",
    "                              improve migration flows, \n",
    "                              model classification was: cohesion between people\n",
    "\n",
    "No/But was correct later ->   user input was: save the planet\n",
    "                              model classification was: safety (the safety of air travel or the environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb9d12",
   "metadata": {},
   "source": [
    "## User Confirmation Rates Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771a663",
   "metadata": {},
   "source": [
    "Incorrect user responses were those that either didn't make much sense or involved multiple ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7a67e",
   "metadata": {},
   "source": [
    "### Question-level user confirmation rate (after dropping incorrect user responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c1eb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_data = df[df['MainCategoryCorrect'] != 'UserInputIncorrect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "789572bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.09859154929578"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_level = (\n",
    "    correct_data.groupby(['SurveySessionID', 'QuestionID'])['IsUserConfirmed']\n",
    "      .max()  # True if any attempt was confirmed\n",
    ")\n",
    "      \n",
    "confirmed_n = question_level.sum()\n",
    "total = question_level.count()\n",
    "\n",
    "question_conf_rate = confirmed_n / total * 100\n",
    "question_conf_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b848a7",
   "metadata": {},
   "source": [
    "### First-attempt confirmation rate (after dropping incorrect user responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8d28ca85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.112676056338024"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_attempts = (\n",
    "    df[(df['UserInputCorrect'] == True) & (df['AttemptID'] == 1)] \n",
    ")\n",
    "\n",
    "confirmed_n = first_attempts['IsUserConfirmed'].sum()\n",
    "total = first_attempts['IsUserConfirmed'].count()\n",
    "\n",
    "question_conf_rate = confirmed_n / total * 100\n",
    "question_conf_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba8c3b",
   "metadata": {},
   "source": [
    "### Failure reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "53eed4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FailureReason\n",
       "AI Correct/Dropped           3\n",
       "AI Correct/Subclass Issue    3\n",
       "AI Correct/Ingenuine User    2\n",
       "Unclear/Not nuanced          2\n",
       "AI Correct/Not nuanced       1\n",
       "AI Correct/Multiple Ideas    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = df[\n",
    "    (df['AttemptID'] == 1)\n",
    "]\n",
    "filtered_data['FailureReason'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b73e9",
   "metadata": {},
   "source": [
    "<b>AI Correct/Dropped</b> -> 3 cases where the AI model returned the closest matching category at least for the main classification, but user did not confirm and has not completed at least 3 attempts [1]\n",
    "\n",
    "<b>AI Correct/Subclass Issue</b> -> 3 cases had a problem with providing a reasonable subclassification despite the correct main classification [2]\n",
    "\n",
    "<b>AI Correct/Ingenuine User</b> -> 1. user input was: sushi, model classification was: Unknown/Other \n",
    "                                    2. user input was: we need a phone, model classification was: Unknown/Other \n",
    "\n",
    "<b>Unclear/Not nuanced</b> -> 1. user input was: create less trafic, model classification was: Unknown / Other [3]\n",
    "                              2. user input was: create sustainable migration channels\n",
    "                              create sustainable and fair migration systems\n",
    "                              improve migration flows, \n",
    "                              model classification was: cohesion between people  [4]\n",
    "\n",
    "<b>AI Correct/Not nuanced</b> -> Seemed relatively good to me, but maybe user did not like it\n",
    "                                 user input was: \n",
    "                                 i think they need to get their shit together and listen to what their citizens want\n",
    "                                 they need to listen to their citizens and help everyone live a good life\n",
    "                                 model classification was:\n",
    "                                 standards of living (improving access to something\tcitizens' needs/improving the quality of something\tquality of life)\n",
    "\n",
    "<b>AI Correct/Multiple Ideas</b> -> user input was:\n",
    "                                    A lot of communication and to be kind to everyone, we also can't forget about nature and te climat\n",
    "                                    model classification was:\n",
    "                                    environment (protecting something\tnature/nature and climate/the environment) [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5175ed11",
   "metadata": {},
   "source": [
    "1. The first 3 cases show that there could be situations where users just dropped out half way through the survey for some reason and it's hard to asses why they haven't completed it. \n",
    "2. At least three cases had issue with subclassification (there were also issues in the cases when the user eventually confirmed)\n",
    "3. One case suggests that a user might not want to confirm when presented with Unknown/Other, meaning that there was also no right category to match\n",
    "4. In this case, subclassification was not available, which might have improve the outcome. Still \"cohesion between people\" seems quite far to the idea of migration, but it's probably the closest (human rights could also work)\n",
    "5. One case that wasn't confirmed had two ideas in it (hence maybe the user didn't like that the first idea was disregarded), but the subclassification sounded also redundant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
