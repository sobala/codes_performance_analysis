{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b070a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ccc49ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../CODES 20250616T141033_raw_data_20240601_20250616.xlsx\", sheet_name=\"belgian_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41867416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SurveyID', 'SurveySessionID', 'SessionStartTime', 'SessionEndTime',\n",
       "       'SessionFeedback1', 'SessionFeedback2', 'ResponseId', 'QuestionID',\n",
       "       'SubQuestionID', 'QuestionText', 'ResponseText', 'ResponseOptionId',\n",
       "       'ClassificationAttempts', 'ClassificationOptionID',\n",
       "       'ClassificationText', 'PercentageLikelihood', 'IsUserConfirmed',\n",
       "       'SubclassificationOptionID', 'SubclassificationText', 'ActionPhrase',\n",
       "       'SubclassificationPercentageLikelihood',\n",
       "       'SubclassificationIsUserConfirmed', 'Unnamed: 22', 'Unnamed: 23',\n",
       "       'Unnamed: 24', 'MainCategoryCorrect', 'AttemptID', 'Unnamed: 27',\n",
       "       'UserConfirmedQ', 'UserInputCorrect', 'FailureReason',\n",
       "       'SubclassPresent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d57b2c",
   "metadata": {},
   "source": [
    "## User Confirmation Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b00d8",
   "metadata": {},
   "source": [
    "### Attempt-level confirmation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "65f41ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.16831683168317"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirmed_attempts = (df['IsUserConfirmed'] == True).sum()\n",
    "total = df['IsUserConfirmed'].count()\n",
    "attempt_conf_rate = confirmed_attempts / total * 100\n",
    "attempt_conf_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9692a40",
   "metadata": {},
   "source": [
    "Users who make many repeated attempts can skew the metric.\n",
    "It doesnâ€™t tell you if the same question was eventually confirmed later.\n",
    "It mixes model performance with user persistence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513289e1",
   "metadata": {},
   "source": [
    "### Question-level confirmation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4f1f755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.48780487804879"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_level = (\n",
    "    df.groupby(['SurveySessionID', 'QuestionID'])['IsUserConfirmed']\n",
    "      .max()  # True if any attempt was confirmed\n",
    ")\n",
    "\n",
    "question_conf_rate = (question_level == True).sum() / question_level.count() * 100\n",
    "question_conf_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7683e3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       82\n",
       "unique       2\n",
       "top       True\n",
       "freq        66\n",
       "Name: IsUserConfirmed, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_level.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff05d232",
   "metadata": {},
   "source": [
    "### User-level confirmation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9747f7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-level confirmation rate: 84.21%\n"
     ]
    }
   ],
   "source": [
    "user_level = (\n",
    "    df.groupby('SurveySessionID')['IsUserConfirmed']\n",
    "      .max()  # True if any attempt for any question was confirmed\n",
    ")\n",
    "\n",
    "user_conf_rate = (user_level == True).sum() / user_level.count() * 100\n",
    "print(f\"User-level confirmation rate: {user_conf_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f777f",
   "metadata": {},
   "source": [
    "### First-attempt confirmation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6b7057b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.21951219512195"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_attempts = df[df['AttemptID'] == 1]\n",
    "confirmed_first_attempts = first_attempts['IsUserConfirmed'].sum()\n",
    "total = first_attempts['IsUserConfirmed'].count()\n",
    "\n",
    "first_attempt_conf_rate = confirmed_first_attempts / total * 100\n",
    "first_attempt_conf_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a92b1",
   "metadata": {},
   "source": [
    "## AI Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9969b10a",
   "metadata": {},
   "source": [
    "### AI main category classification accuracy (based on the first attempts and after dropping incorrect user responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "447df03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.7536231884058"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapsed_data = df[\n",
    "    (df['AttemptID'] == 1) &\n",
    "    (df['MainCategoryCorrect'] != 'UserInputIncorrect')\n",
    "]\n",
    "\n",
    "correct_n = (collapsed_data['MainCategoryCorrect'] == 'Yes').sum()\n",
    "total_n = (collapsed_data['MainCategoryCorrect']).count()\n",
    "\n",
    "main_accuracy = correct_n / total_n * 100\n",
    "main_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "883b665e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MainCategoryCorrect\n",
       "Yes                         64\n",
       "Not sure/Old label used      2\n",
       "Not sure/\"Unknown\" label     1\n",
       "Not sure/But seems close     1\n",
       "No/But was correct later     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapsed_data['MainCategoryCorrect'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7a67e",
   "metadata": {},
   "source": [
    "### Question-level user confirmation rate (after dropping incorrect user responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c1eb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_data = df[df['MainCategoryCorrect'] != 'UserInputIncorrect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "84c6be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_level = (\n",
    "    correct_data.groupby(['SurveySessionID', 'QuestionID'])['IsUserConfirmed']\n",
    "      .max()  # True if any attempt was confirmed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "789572bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.09859154929578"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirmed_n = question_level.sum()\n",
    "total = question_level.count()\n",
    "\n",
    "question_conf_rate = confirmed_n / total * 100\n",
    "question_conf_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b848a7",
   "metadata": {},
   "source": [
    "### First-attempt confirmation rate (after dropping incorrect user responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_data = df[\n",
    "    (df['AttemptID'] == 1) &\n",
    "    (df['MainCategoryCorrect'] != 'UserInputIncorrect')\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "first_attempts = df[df['AttemptID'] == 1]\n",
    "confirmed_first_attempts = first_attempts['IsUserConfirmed'].sum()\n",
    "total = first_attempts['IsUserConfirmed'].count()\n",
    "\n",
    "first_attempt_conf_rate = confirmed_first_attempts / total * 100\n",
    "first_attempt_conf_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8d28ca85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.09859154929578"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_level_dropped_incorrect = (\n",
    "    df[(df['UserInputCorrect'] == True) & (df['AttemptID'] == 1)]\n",
    "    .groupby(['SurveySessionID', 'QuestionID'])['UserConfirmedQ']\n",
    "    .first()  # or .max(), same since each group should have one attempt\n",
    ")\n",
    "\n",
    "question_conf_rate = (question_level_dropped_incorrect == True).sum() / question_level_dropped_incorrect.count() * 100\n",
    "question_conf_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba8c3b",
   "metadata": {},
   "source": [
    "### Failure reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "53eed4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FailureReason\n",
       "AI Correct/Dropped           3\n",
       "AI Correct/Subclass Issue    3\n",
       "AI Correct/Ingenuine User    2\n",
       "Unclear/Not nuanced          2\n",
       "AI Correct/Not nuanced       1\n",
       "AI Correct/Multiple Ideas    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = df[\n",
    "    (df['AttemptID'] == 1)\n",
    "]\n",
    "filtered_data['FailureReason'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b73e9",
   "metadata": {},
   "source": [
    "<b>AI Correct/Dropped</b> -> 3 cases where the AI model returned the closest matching category at least for the main classification, but user did not confirm and has not completed at least 3 attempts [1]\n",
    "\n",
    "<b>AI Correct/Subclass Issue</b> -> 3 cases had a problem with providing a reasonable subclassification despite the correct main classification [2]\n",
    "\n",
    "<b>AI Correct/Ingenuine User</b> -> 1. user input was: sushi, model classification was: Unknown/Other \n",
    "                                    2. user input was: we need a phone, model classification was: Unknown/Other \n",
    "\n",
    "<b>Unclear/Not nuanced</b> -> 1. user input was: create less trafic, model classification was: Unknown / Other [3]\n",
    "                              2. user input was: create sustainable migration channels\n",
    "                              create sustainable and fair migration systems\n",
    "                              improve migration flows, \n",
    "                              model classification was: cohesion between people  [4]\n",
    "\n",
    "<b>AI Correct/Not nuanced</b> -> Seemed relatively good to me, but maybe user did not like it\n",
    "                                 user input was: \n",
    "                                 i think they need to get their shit together and listen to what their citizens want\n",
    "                                 they need to listen to their citizens and help everyone live a good life\n",
    "                                 model classification was:\n",
    "                                 standards of living (improving access to something\tcitizens' needs/improving the quality of something\tquality of life)\n",
    "\n",
    "<b>AI Correct/Multiple Ideas</b> -> user input was:\n",
    "                                    A lot of communication and to be kind to everyone, we also can't forget about nature and te climat\n",
    "                                    model classification was:\n",
    "                                    environment (protecting something\tnature/nature and climate/the environment) [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5175ed11",
   "metadata": {},
   "source": [
    "1. The first 3 cases show that there could be situations where users just dropped out half way through the survey for some reason and it's hard to asses why they haven't completed it. \n",
    "2. At least three cases had issue with subclassification (there were also issues in the cases when the user eventually confirmed)\n",
    "3. One case suggests that a user might not want to confirm when presented with Unknown/Other, meaning that there was also no right category to match\n",
    "4. In this case, subclassification was not available, which might have improve the outcome. Still \"cohesion between people\" seems quite far to the idea of migration, but it's probably the closest (human rights could also work)\n",
    "5. One case that wasn't confirmed had two ideas in it (hence maybe the user didn't like that the first idea was disregarded), but the subclassification sounded also redundant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c160ec5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
